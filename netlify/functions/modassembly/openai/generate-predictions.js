/**
 * File generated by Modular Assembly
 * IMPORTANT!!! Ask the user before editing this file.
 */

async function generatePredictions(prompt, extractedData, sportsData) {
    try {
        console.log(`ðŸ¤– Generating response with OpenAI for prompt: "${prompt}"`);

        const MAX_PROMPT_LENGTH = 25000;
        const sport = extractedData.sport;
        const betType = extractedData.bet_type || 'general';
        const riskProfile = extractedData.risk_profile || 'moderate';

        // Get current time in NYC timezone
        const nycTime = new Date().toLocaleString('en-US', {
            timeZone: 'America/New_York',
            hour: 'numeric',
            minute: 'numeric',
            hour12: true
        });
        // Limit the sportsData stringified length as well
        let sportsDataString = JSON.stringify(sportsData, null, 2);
        if (sportsDataString.length > MAX_PROMPT_LENGTH) {
            sportsDataString = sportsDataString.slice(0, MAX_PROMPT_LENGTH) + '\n... [truncated]';
        }
        const messages = [
            {
                role: 'system',
                content: `You are a sports betting assistant for the ${sport}. The user prompt asks for a ${betType} pick using the provided data.

Use only the given schedule, scores, and standings. Suggest picks from *future games only*. Format clearly and briefly.

If the user asks for props, include 2â€“3 relevant stat-based props.

If data is insufficient for a confident pick, explain whatâ€™s missing.

Todayâ€™s date in NYC is ${nycTime}.
         `
            },
            {
                role: 'user',
                content: `My prompt: ${prompt}\n\nHere's the relevant sports data:\n${sportsDataString}`
            }
        ];

        const baseUrl = process.env.URL || process.env.DEPLOY_URL || 'http://localhost:8888';
        const response = await fetch(`${baseUrl}/.netlify/functions/openai`, {
            method: 'POST', // Increase max tokens for more detailed response
            body: JSON.stringify({
                messages,
                use_claude: true,  // Use OpenAI
                max_tokens: 500   // Increase max tokens for more detailed response
            })
        });
        console.log(`ðŸ“¥ OpenAI response status: ${response.status}`);
        const data = await response.json();

        if (!data.message) {
            throw new Error('Failed to generate response');
        }

        return {
            predictionsText: data.message.content,
        };
    } catch (error) {
        console.error('Error generating response:', error);
        throw error;
    }
}

module.exports = {
    generatePredictions
};
